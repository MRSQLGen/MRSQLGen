We evaluate two LLM-as-a-Judge settings: LAJ-Self, where the model evaluates its own outputs, and LAJ-GPT5, which uses a stronger external judge. LLMs self-judging tends to overlook their own hallucinations , suggesting that LAJ-Self can suffer from self-evaluation limitations and may underestimate certain errors. A stronger model is therefore needed to assess how much of the reported improvements are due to MRSQLGen’s actual strengths versus the inherent limitations of self-judging, rather than genuine methodological advantages.

The experimental setup is the same as in Section 7.1 “Experimental Setup” of the paper, with the addition of a stronger judge model, GPT-5.1.The results are listed as follows.

|             |        |       |          |            |     |             |       |          |            |     |                        |       |          |            |     |                       |       |          |            |     |                   |       |          |            |
| ----------- | ------ | ----- | -------- | ---------- | --- | ----------- | ----- | -------- | ---------- | --- | ---------------------- | ----- | -------- | ---------- | --- | --------------------- | ----- | -------- | ---------- | --- | ----------------- | ----- | -------- | ---------- |
|             | gpt-4o |       |          |            |     | gpt-4o-mini |       |          |            |     | CodeLlama-13B-Instruct |       |          |            |     | Deepseek-Coder-v2:16B |       |          |            |     | Qwen2.5-Coder:14B |       |          |            |
|             | MRS    | SCG   | LAJ-SELF | LAJ-GPT5.1 |     | MRS         | SCG   | LAJ-SELF | LAJ-GPT5.1 |     | MRS                    | SCG   | LAJ-SELF | LAJ-GPT5.1 |     | MRS                   | SCG   | LAJ-SELF | LAJ-GPT5.1 |     | MRS               | SCG   | LAJ-SELF | LAJ-GPT5.1 |
| Spider (P)  | 0.782  | 0.200 | 0.357    | 0.716      |     | 0.651       | 0.294 | 0.577    | 0.571      |     | 0.687                  | 0.358 | 0.276    | 0.257      |     | 0.478                 | 0.454 | 0.366    | 0.205      |     | 0.651             | 0.291 | 0.666    | 0.167      |
| Spider (R)  | 0.560  | 0.083 | 0.074    | 0.369      |     | 0.690       | 0.059 | 0.180    | 0.400      |     | 0.973                  | 0.372 | 0.225    | 1.000      |     | 0.883                 | 0.187 | 0.317    | 1.000      |     | 0.754             | 0.112 | 0.089    | 1.000      |
| Spider (F1) | 0.653  | 0.117 | 0.123    | 0.487      |     | 0.670       | 0.099 | 0.275    | 0.470      |     | 0.805                  | 0.365 | 0.248    | 0.409      |     | 0.620                 | 0.265 | 0.339    | 0.340      |     | 0.699             | 0.162 | 0.157    | 0.286      |
| BIRD (P)    | 0.696  | 0.666 | 0.619    | 0.604      |     | 0.803       | 0.564 | 0.562    | 0.623      |     | 0.905                  | 0.280 | 0.302    | 0.351      |     | 0.816                 | 0.583 | 0.565    | 0.519      |     | 0.701             | 0.621 | 0.545    | 0.531      |
| BIRD (R)    | 0.692  | 0.124 | 0.349    | 0.454      |     | 0.714       | 0.108 | 0.407    | 0.617      |     | 0.986                  | 0.421 | 0.220    | 0.791      |     | 0.933                 | 0.241 | 0.278    | 0.717      |     | 0.818             | 0.250 | 0.152    | 0.558      |
| BIRD (F1)   | 0.694  | 0.209 | 0.446    | 0.518      |     | 0.756       | 0.182 | 0.472    | 0.620      |     | 0.944                  | 0.336 | 0.254    | 0.486      |     | 0.870                 | 0.341 | 0.373    | 0.602      |     | 0.755             | 0.356 | 0.238    | 0.544      |

Our findings show that LAJ-GPT5 yields no notable F1 gains over LAJ-Self, suggesting that self-judging may already approximate the upper bound of current LLM-as-a-Judge reliability for this task.